{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to Make a Shot Quality Model\n",
    "\n",
    "### What is a Shot Quality Model\n",
    "\n",
    "In general, a shot quality model's goal is to predict how many points a player will score based on a shot (PPS). There are three kinds of shots a player can take, FT, FG2, and FG3. You could make three separate models or one model that works for all three kinds of shots. One nice and/or challenging aspect of these models is that you're more or less entirely bound between 0-3. If your PPS model is predicting 7, there might be an issue. \n",
    "\n",
    "One well documented public example of a good shot quality model is KOBE, developed by [Krisha Narsu](http://twitter.com/knarsu3) and [published at Nylon Calculus](https://fansided.com/2015/09/28/introducing-kobe-a-measure-of-shot-quality/). The model we develop here will not be as good, but will also not be named after a Lakers player, so that's a plus. In fact, we can go ahead and name ours **EMBIID**, or **E**xtremely **M**ediocre to **B**ad **I**ntroductory scor**I**ng mo**D**el.\n",
    "\n",
    "We're going to be using exclusively shot data derived from play-by-play data - no tracking or demographic info here. The shots are 100,000 random fieldgoal attempts from the 2018 and 2019 regular seasons. \n",
    "\n",
    "### Get the Data Set Up\n",
    "\n",
    "We're going to read in the data first and just sanity check that it looks like shot data. Secondly, as this is spatial data, we're going to visualize some shots. I cannot emphasize enough how important it is to be 10000% confident in your coordinate system when you use spatial data. Spending 8hrs modeling and then realizing you need to change the CRS or do a bunch of rotations is not a good feeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_shots = pd.read_csv(\"data/shots.csv\")\n",
    "print(all_shots.head())\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "#plt.axis('equal')\n",
    "plt.xlim(-25, 25)\n",
    "plt.ylim(-4, 90)\n",
    "plt.title(\"Shots!\")\n",
    "sns.scatterplot(\n",
    "    data=all_shots, \n",
    "    x=\"x\", \n",
    "    y=\"y\", \n",
    "    hue=\"shot_type\",\n",
    "    alpha = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that certainly looks correct or close enough. However, there are quite a few heaves that we probably want to take out of the model since we probably(?) don't care about those. Let's limit it to shots 32 feet and closer (the top line of the little court). We only lose about 600 shots by doing this, so shouldn't be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_shots.shape)\n",
    "\n",
    "shots = all_shots.loc[all_shots['shot_distance'] <= 32]\n",
    "\n",
    "print(shots.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Modeling Interlude\n",
    "\n",
    "Before we get started, a brief soapbox moment. It can be tempting to fire up TensorFlow and get crazy here, but starting simple and improving your model iteratively is a good idea. Go small to big, as you might find out you don't need a 40 layer NN to get the job done. (Although sometimes you do).\n",
    "\n",
    "## Modeling\n",
    "\n",
    "The good news about a PPS model is that there is an extremely obvious first pass model to either test or at least consider. Very slight math warning approaching.\n",
    "\n",
    "$xPPS = {\\beta_0} + {\\beta_1} * FG2 + {\\beta_2} * FG3$\n",
    "\n",
    "This is our starting point. This model takes binary flags for FG2 and FG3 and then returns an expected point per shot value (xPPS). Although, let's consider if we actually need an intercept here - what would the xPPS be if FG2 = 0 and FG3 = 0? It would be zero, so we can actually get rid of the intercept entirely. You could of course also remove one of FG2 or FG3 as well and keep the intercept, but I like it this way for presentation purposes. EMBIID v1.0 is presented below.\n",
    "\n",
    "$xPPS = {\\beta_1} * FG2 + {\\beta_2} * FG3$\n",
    "\n",
    "\n",
    "Now, let's use this model setup to train on 75% of the data and test on 25% of the data and see how we do. Sidenote, I'm doing this with statsmodels because sklearn doesn't have linear regression summary tables which makes me insane."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 points   R-squared:                      -0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    -1.097\n",
      "Date:                Tue, 05 Jan 2021   Prob (F-statistic):               1.00\n",
      "Time:                        13:10:24   Log-Likelihood:            -1.1833e+05\n",
      "No. Observations:               74543   AIC:                         2.367e+05\n",
      "Df Residuals:                   74540   BIC:                         2.367e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -3.111e+11   3.02e+11     -1.031      0.303   -9.03e+11    2.81e+11\n",
      "x1          3.111e+11   3.02e+11      1.031      0.303   -2.81e+11    9.03e+11\n",
      "x2          3.111e+11   3.02e+11      1.031      0.303   -2.81e+11    9.03e+11\n",
      "==============================================================================\n",
      "Omnibus:                   386245.890   Durbin-Watson:                   2.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8887.496\n",
      "Skew:                           0.316   Prob(JB):                         0.00\n",
      "Kurtosis:                       1.431   Cond. No.                     1.50e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.12e-24. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "shots['fg2'] = np.where(shots['shot_type'] == '2PT Field Goal', 1, 0)\n",
    "shots['fg3'] = np.where(shots['shot_type'] == '3PT Field Goal', 1, 0)\n",
    "\n",
    "train_shots, test_shots = train_test_split(shots, train_size=0.75)\n",
    "\n",
    "## NOTE THAT Y COMES FIRST UNLIKE SKLEARN\n",
    "\n",
    "y_train = train_shots['points']\n",
    "X_train = train_shots[['fg2', 'fg3']].to_numpy()\n",
    "X_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "shots['fg2'] = np.where(shots['shot_type'] == '2PT Field Goal', 1, 0)\n",
    "shots['fg3'] = np.where(shots['shot_type'] == '3PT Field Goal', 1, 0)\n",
    "\n",
    "train_shots, test_shots = train_test_split(shots, train_size=0.75)\n",
    "\n",
    "## NOTE THAT Y COMES FIRST UNLIKE SKLEARN\n",
    "model = sm.OLS(y=train_shots['points'].to_numpy(), x=train_shots[['fg2', 'fg3'].to_numpy()])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}